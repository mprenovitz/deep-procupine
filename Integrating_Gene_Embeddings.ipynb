{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f44b90b5",
      "metadata": {
        "id": "f44b90b5",
        "outputId": "61e3d9dd-6330-4a3f-d9f5-78bfa79b6596"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/drraghav/.conda/envs/procyon/lib/python3.11/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-07 13:52:44,575] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/users/drraghav/.conda/envs/procyon/lib/python3.11/site-packages/wandb/sdk/launch/builder/build.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from unittest.mock import MagicMock\n",
        "\n",
        "# Mock flash_attn module\n",
        "sys.modules['flash_attn'] = MagicMock()\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from procyon.data.inference_utils import (\n",
        "    create_caption_input_simple,\n",
        "    create_qa_input_simple,\n",
        "    uniprot_id_to_index,\n",
        "    ProCyonQAInference,\n",
        ")\n",
        "from procyon.model.model_unified import UnifiedProCyon\n",
        "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17bdc3f1",
      "metadata": {
        "id": "17bdc3f1",
        "outputId": "784f710b-9b7e-40e4-b9a5-33a14b9cdd44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating model args DATA_DIR from /n/holystore01/LABS/mzitnik_lab/Lab/PLM -> /oscar/data/rsingh47/drraghav/procupine/ProCyon-Instruct\n",
            "updating stale DATA_DIR for model arg: go_embeddings_path\n",
            "updating stale DATA_DIR for model arg: pfam_embeddings_path\n",
            "updating stale DATA_DIR for model arg: drugbank_embeddings_path\n",
            "updating stale DATA_DIR for model arg: reactome_embeddings_path\n",
            "updating stale DATA_DIR for model arg: omim_embeddings_path\n",
            "updating stale DATA_DIR for model arg: ec_embeddings_path\n",
            "updating stale DATA_DIR for model arg: protein_seq_embeddings_path\n",
            "updating stale DATA_DIR for model arg: protein_struct_embeddings_path\n",
            "updating stale DATA_DIR for model arg: protein_embeddings_idmap_path\n",
            "updating stale DATA_DIR for model arg: drug_struct_embeddings_path\n",
            "updating stale DATA_DIR for model arg: domain_embeddings_path\n",
            "updating stale DATA_DIR for model arg: domain_embeddings_idmap_path\n",
            "updating stale DATA_DIR for model arg: mouse_ortholog_embeddings_path\n",
            "updating stale DATA_DIR for model arg: mouse_ortholog_embeddings_idmap_path\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing zero checkpoint '/oscar/data/rsingh47/drraghav/procupine/ProCyon-Full/global_step59469'\n",
            "Detected checkpoint of type zero stage ZeroStageEnum.gradients, world_size: 32\n",
            "Parsing checkpoint created by deepspeed==0.12.4\n",
            "Reconstructed fp32 state dict with 322 params 8141117441 elements\n",
            "Text Encoder is on: cuda:0\n",
            "Protein Embeddings are on: cuda:0\n"
          ]
        }
      ],
      "source": [
        "from accelerate import dispatch_model\n",
        "# -------------------------\n",
        "# 1. Load checkpoint & args\n",
        "# -------------------------\n",
        "checkpoint_path = \"/oscar/data/rsingh47/drraghav/procupine/ProCyon-Full\"\n",
        "CKPT_NAME = checkpoint_path\n",
        "\n",
        "data_args = torch.load(os.path.join(CKPT_NAME, \"data_args.pt\"))\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model, _ = UnifiedProCyon.from_pretrained(checkpoint_dir=CKPT_NAME)\n",
        "model.eval()\n",
        "model.bfloat16()\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2. Generate initial device map using accelerate\n",
        "# --------------------------------------------------------\n",
        "# This gives us a starting point, but we will override\n",
        "device_map = infer_auto_device_map(\n",
        "    model,\n",
        "    max_memory={0: \"20GB\", 1: \"20GB\"},\n",
        "    no_split_module_classes=[\"LlamaDecoderLayer\"]\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3. Force text encoder fully on GPU0\n",
        "# --------------------------------------------------------\n",
        "fixed_map = dict(device_map)\n",
        "\n",
        "for k in fixed_map.keys():\n",
        "    if k.startswith(\"text_encoder\"):\n",
        "        fixed_map[k] = 0\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 4. Put protein modules on GPU1\n",
        "# --------------------------------------------------------\n",
        "protein_modules = [\n",
        "    \"protein_seq_embeddings\",\n",
        "    \"protein_struct_embeddings\",\n",
        "    \"domain_embeddings\",\n",
        "    \"token_projectors\",\n",
        "    \"drug_structure_embeddings\",\n",
        "    \"aaseq_shared_projector\",\n",
        "    \"aaseq_lm_projector\",\n",
        "    \"contrastive_head\",\n",
        "]\n",
        "\n",
        "for k in protein_modules:\n",
        "    if k in fixed_map:\n",
        "        fixed_map[k] = 1\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 5. Make sure input embeddings are on GPU0\n",
        "# --------------------------------------------------------\n",
        "fixed_map[\"input_embeddings\"] = 0\n",
        "fixed_map[\"text_encoder.model.lm_head\"] = 0\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 6. Dispatch model according to this fixed_map\n",
        "# --------------------------------------------------------\n",
        "model = dispatch_model(model, fixed_map)\n",
        "model.eval()\n",
        "model.bfloat16()\n",
        "\n",
        "print(f\"Text Encoder is on: {model.text_encoder.model.device}\")\n",
        "print(f\"Protein Embeddings are on: {model.protein_seq_embeddings.weight.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72cfd16d",
      "metadata": {
        "id": "72cfd16d"
      },
      "outputs": [],
      "source": [
        "## This runs the Procyon inference model\n",
        "\n",
        "random_idx = 1\n",
        "sample_row = df_clean.iloc[random_idx]\n",
        "gene_name = sample_row['gene_name']\n",
        "sample_prot_id = int(sample_row['procyon_id'])\n",
        "protein_ids = [sample_prot_id]\n",
        "input_simple = create_caption_input_simple(\n",
        "    input_aaseq_ids=protein_ids,\n",
        "    data_args=data_args,\n",
        "    # The `instruction_source_dataset` and `instruction_source_relation` here control the style\n",
        "    # of pre-templated instruction used in these queries. In particular, here we query for UniProt-style\n",
        "    # functional descriptions.\n",
        "    instruction_source_dataset=\"uniprot\",\n",
        "    instruction_source_relation=\"all\",\n",
        "    aaseq_type=\"protein\",\n",
        "    task_type=\"caption\",\n",
        "    icl_example_number=1,\n",
        "     device=device,\n",
        ")\n",
        "def move_to_device(obj, device=\"cuda:0\"):\n",
        "    if torch.is_tensor(obj):\n",
        "        return obj.to(device)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: move_to_device(v, device) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [move_to_device(x, device) for x in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "input_simple = move_to_device(input_simple, \"cuda:0\")\n",
        "\n",
        "text_gen_args = {\n",
        "    \"method\": \"beam\",\n",
        "    # Maximum length of generated text.\n",
        "    \"max_len\": 200,\n",
        "    # Total number of beams maintained per input. `beam_size` / `beam_group_size` = number of phenotypes returned per input.\n",
        "    \"beam_size\": 20,\n",
        "    # Size of the individual beam groups in DBS.\n",
        "    \"beam_group_size\": 2,\n",
        "    # Penalty applied to repetition within a beam group.\n",
        "    \"diversity_penalty\": 0.8,\n",
        "}\n",
        "\n",
        "out_tokens, log_probs, output_logits, out_text = model.generate(\n",
        "    inputs=input_simple,\n",
        "    aaseq_type=\"protein\",\n",
        "    **text_gen_args\n",
        ")\n",
        "output_phenotypes = [\n",
        "    phen for i, phen in enumerate(out_text[0]) if i % text_gen_args[\"beam_group_size\"] == 0\n",
        "]\n",
        "qa_model = ProCyonQAInference(model, device=device)\n",
        "\n",
        "# Try QA filtering\n",
        "results = []\n",
        "for i, query_text in enumerate(output_phenotypes):\n",
        "    input_qa_simple = create_qa_input_simple(\n",
        "        input_aaseq_ids=protein_ids,\n",
        "        data_args=data_args,\n",
        "        input_description=query_text,\n",
        "        instruction_source_dataset=\"uniprot\",\n",
        "        instruction_source_relation=\"all\",\n",
        "        aaseq_type=\"protein\",\n",
        "        icl_example_number=1,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_qa_out = qa_model(input_qa_simple)\n",
        "\n",
        "    yes_prob = model_qa_out[\"pred\"][0, qa_model.yes_token].item()\n",
        "    no_prob = model_qa_out[\"pred\"][0, qa_model.no_token].item()\n",
        "\n",
        "\n",
        "    print(f\"TEXT {i} --------------------------------------------\")\n",
        "    print(query_text)\n",
        "    print(f\"Yes: {yes_prob:0.3f}\")\n",
        "    print(f\"No: {no_prob:0.3f}\")\n",
        "\n",
        "    results.append({\n",
        "        \"phenotype\": query_text,\n",
        "        \"yes_prob\": yes_prob\n",
        "    })\n",
        "\n",
        "results = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c63321df",
      "metadata": {
        "id": "c63321df",
        "outputId": "6ace4458-65c5-4a35-a2c7-dae2f484c816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pbmk.csv...\n",
            "Filtered 40 rows with missing data. Remaining: 636\n",
            "Parsing embedding column: 'biogpt_embedding'...\n",
            "Detected Embedding Dimension: 1024\n",
            "Mapping UniProt IDs to ProCyon Indices...\n",
            "Final Count: 116 samples ready for training.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "CSV_PATH = \"pbmk.csv\"\n",
        "EMBEDDING_COL = \"biogpt_embedding\"\n",
        "TEXT_COL = \"summary\"\n",
        "ID_COL = \"uniprot_id\"\n",
        "GENE_COL = \"gene_name\"\n",
        "\n",
        "# Expected Dimensions\n",
        "INPUT_DIM = 1024 # BioGPT dimension\n",
        "LLAMA_DIM = 4096\n",
        "\n",
        "def clean_and_load_data(csv_path):\n",
        "    print(f\"Loading {csv_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found!\")\n",
        "        return None, None\n",
        "\n",
        "    # 1. Filter Missing Data\n",
        "    initial_len = len(df)\n",
        "    df = df.dropna(subset=[ID_COL, TEXT_COL, EMBEDDING_COL])\n",
        "    print(f\"Filtered {initial_len - len(df)} rows with missing data. Remaining: {len(df)}\")\n",
        "\n",
        "    # 2. Parse Embeddings\n",
        "    print(f\"Parsing embedding column: '{EMBEDDING_COL}'...\")\n",
        "    def parse_embedding(x):\n",
        "        try:\n",
        "            # Handle string format \"0.0,0.0,0.0\"\n",
        "            if isinstance(x, str):\n",
        "                clean_str = x.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", \"\")\n",
        "                return np.fromstring(clean_str, sep=',')\n",
        "            return np.array(x)\n",
        "        except:\n",
        "            return np.zeros(INPUT_DIM)\n",
        "\n",
        "    df['vec'] = df[EMBEDDING_COL].apply(parse_embedding)\n",
        "\n",
        "    # 3. Detect Dimension\n",
        "    valid_vecs = df[df['vec'].apply(len) > 1]['vec']\n",
        "    if len(valid_vecs) > 0:\n",
        "        actual_dim = len(valid_vecs.iloc[0])\n",
        "    else:\n",
        "        actual_dim = INPUT_DIM\n",
        "    print(f\"Detected Embedding Dimension: {actual_dim}\")\n",
        "\n",
        "    # 4. Map UniProt IDs to ProCyon Indices\n",
        "    print(\"Mapping UniProt IDs to ProCyon Indices...\")\n",
        "\n",
        "    valid_indices = []\n",
        "    valid_rows = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        uid = row[ID_COL]\n",
        "        try:\n",
        "            # Try to map using ProCyon's function\n",
        "            # If the ID isn't in ProCyon's vocab, this might fail, so we catch it\n",
        "            procyon_idx = uniprot_id_to_index(uid)\n",
        "            valid_indices.append(procyon_idx)\n",
        "            valid_rows.append(idx)\n",
        "        except Exception:\n",
        "            # ID not found in ProCyon database - skip it\n",
        "            continue\n",
        "\n",
        "    # Filter DF to only include mappable proteins\n",
        "    df_mapped = df.loc[valid_rows].copy()\n",
        "    df_mapped['procyon_id'] = valid_indices\n",
        "\n",
        "    print(f\"Final Count: {len(df_mapped)} samples ready for training.\")\n",
        "    return df_mapped, actual_dim\n",
        "\n",
        "df_clean, DETECTED_DIM = clean_and_load_data(CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2411ef4",
      "metadata": {
        "id": "e2411ef4",
        "outputId": "fa964775-9ade-4229-a38b-fc544cd72560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to load tokenizer from: /oscar/data/rsingh47/drraghav/procupine/ProCyon-Instruct/model_weights/llama-3-8b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed padding token.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Assuming 'checkpoint_path' is defined from the initial model load cell\n",
        "try:\n",
        "    # 1. Fetch the path (robust check)\n",
        "    llama_path = model.text_encoder.model.config._name_or_path\n",
        "except AttributeError:\n",
        "    llama_path = getattr(torch.load(os.path.join(checkpoint_path, \"model_args.pt\")), 'model_name_or_path', None)\n",
        "\n",
        "if llama_path:\n",
        "    print(f\"Attempting to load tokenizer from: {llama_path}\")\n",
        "    # 2. Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n",
        "\n",
        "    # 3. Fix missing pad token (Crucial for attention masks)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"Fixed padding token.\")\n",
        "else:\n",
        "    raise ValueError(\"CRITICAL: Could not find the Llama path to load the tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c540d7",
      "metadata": {
        "id": "51c540d7",
        "outputId": "179d17e2-9097-4fca-8aba-50d66b5283fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instantiating dataset and train_loader...\n",
            "train_loader created with 116 batches.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GeneAdapterDataset(Dataset):\n",
        "    \"\"\"Dataset for aligning Gene Embeddings (vec) to Gene Summary Text (summary).\"\"\"\n",
        "    def __init__(self, df, tokenizer, gene_embedding_col='vec', summary_col='summary', prot_id_col='procyon_id'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.gene_feats = np.stack(df[gene_embedding_col].values)\n",
        "        self.texts = df[summary_col].tolist()\n",
        "        self.prot_ids = df[prot_id_col].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize the target text (Gene Summary)\n",
        "        gene_text = self.texts[idx]\n",
        "        tokenized_output = self.tokenizer(gene_text,\n",
        "                                          padding='max_length',\n",
        "                                          truncation=True,\n",
        "                                          max_length=128,\n",
        "                                          return_tensors='pt')\n",
        "\n",
        "        # Prepare inputs\n",
        "        return {\n",
        "            'gene_feat': self.gene_feats[idx].astype(np.float32),\n",
        "            'prot_id': self.prot_ids[idx],\n",
        "            'gene_text_labels': tokenized_output['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized_output['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Separate lists of numpy arrays and tensors\n",
        "    gene_feats = [item['gene_feat'] for item in batch]\n",
        "    prot_ids = [item['prot_id'] for item in batch]\n",
        "    gene_text_labels = [item['gene_text_labels'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "\n",
        "    # Convert gene features to a torch tensor (already padded by numpy.stack)\n",
        "    gene_feats = torch.tensor(np.stack(gene_feats), dtype=torch.float)\n",
        "    prot_ids = torch.tensor(prot_ids, dtype=torch.long)\n",
        "\n",
        "    # Pad text and mask sequences\n",
        "    gene_text_labels_padded = rnn_utils.pad_sequence(gene_text_labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_masks_padded = rnn_utils.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Return final batch dictionary\n",
        "    return {\n",
        "        'gene_feat': gene_feats,\n",
        "        'prot_id': prot_ids,\n",
        "        'gene_text_labels': gene_text_labels_padded,\n",
        "        'attention_mask': attention_masks_padded,\n",
        "    }\n",
        "\n",
        "print(\"Instantiating dataset and train_loader...\")\n",
        "\n",
        "# Use the cleaned DataFrame (df_clean) and the loaded tokenizer\n",
        "dataset = GeneAdapterDataset(df_clean, tokenizer)\n",
        "\n",
        "# Set batch size to 1 to manage memory (as discussed)\n",
        "# train_loader is the variable needed for the training loop\n",
        "train_loader = DataLoader(dataset,\n",
        "                          batch_size=1,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=collate_fn)\n",
        "\n",
        "print(f\"train_loader created with {len(train_loader)} batches.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5002aab8",
      "metadata": {
        "id": "5002aab8"
      },
      "outputs": [],
      "source": [
        "## TRAIN THE ADAPTER\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# --- TRAINING PARAMETERS ---\n",
        "NUM_EPOCHS = 3\n",
        "ACCUMULATION = 8\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "class GeneAdapter(nn.Module):\n",
        "    def __init__(self, input_dim=1024, output_dim=4096):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(output_dim, output_dim),\n",
        "            nn.LayerNorm(output_dim)\n",
        "        )\n",
        "        for m in self.net.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        out = self.net(x).unsqueeze(1)\n",
        "        out = nn.functional.normalize(out, p=2, dim=-1)\n",
        "        out.requires_grad_(True)\n",
        "        return out\n",
        "\n",
        "class ProCyonIntegrated(nn.Module):\n",
        "    def __init__(self, original_procyon, adapter, tokenizer):\n",
        "        super().__init__()\n",
        "        self.procyon = original_procyon\n",
        "        self.adapter = adapter\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        for p in self.procyon.parameters(): p.requires_grad = False\n",
        "        self.procyon.text_encoder.model.gradient_checkpointing_enable()\n",
        "        for p in self.adapter.parameters(): p.requires_grad = True\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Text Encoder (Llama) is on GPU 0\n",
        "        main_device = torch.device(\"cuda:0\")\n",
        "        prot_device = self.procyon.protein_seq_embeddings.weight.device # GPU 1\n",
        "\n",
        "        # A. Gene Branch (Adapter)\n",
        "        # Adapter and Gene Input MUST be on GPU 0\n",
        "        g_input = batch['gene_feat'].to(main_device).to(torch.bfloat16)\n",
        "        g_embed = self.adapter(g_input)\n",
        "\n",
        "        # B. Protein Branch\n",
        "        p_ids = batch['prot_id'].to(prot_device) # Protein IDs stay on GPU 1\n",
        "        p_raw = self.procyon.protein_seq_embeddings(p_ids)\n",
        "        p_embed = self.procyon.token_projectors['aaseq'](p_raw).to(main_device)\n",
        "        if p_embed.dim() == 2: p_embed = p_embed.unsqueeze(1)\n",
        "\n",
        "        # C. Ground Truth\n",
        "        target_ids = batch['gene_text_labels'].to(main_device)\n",
        "        target_embeds = self.procyon.input_embeddings(target_ids)\n",
        "\n",
        "        # D. The Input Sandwich\n",
        "        inputs_embeds = torch.cat([g_embed, p_embed, target_embeds], dim=1)\n",
        "\n",
        "        # E. Masks & Labels setup\n",
        "        batch_size = len(p_ids)\n",
        "        target_mask = batch['attention_mask'].to(main_device)\n",
        "        ones = torch.ones((batch_size, 2), device=main_device)\n",
        "        full_mask = torch.cat([ones, target_mask], dim=1)\n",
        "        ignore = torch.full((batch_size, 2), -100, device=main_device, dtype=torch.long)\n",
        "        full_labels = torch.cat([ignore, target_ids], dim=1)\n",
        "\n",
        "        return self.procyon.text_encoder.model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=full_mask,\n",
        "            labels=full_labels\n",
        "        )\n",
        "\n",
        "# --- 3. Initialization (Adapter on GPU 0) ---\n",
        "adapter = GeneAdapter(input_dim=DETECTED_DIM, output_dim=LLAMA_DIM).to(\"cuda:0\").bfloat16()\n",
        "\n",
        "wrapper = ProCyonIntegrated(model, adapter, tokenizer)\n",
        "optimizer = optim.AdamW(adapter.parameters(), lr=1e-4)\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "print(f\"\\n--- Starting Integrated Adapter Training (Batch Size 1) ---\")\n",
        "wrapper.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    print(f\"\\n=== Epoch {epoch + 1}/{NUM_EPOCHS} ===\")\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        try:\n",
        "            # Forward\n",
        "            outputs = wrapper(batch)\n",
        "\n",
        "            # Divide loss by accumulation steps\n",
        "            loss = outputs.loss / ACCUMULATION\n",
        "            loss.backward()\n",
        "\n",
        "            # Step (Every 8 batches)\n",
        "            if (i + 1) % ACCUMULATION == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(adapter.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Logging\n",
        "                actual_loss = loss.item() * ACCUMULATION\n",
        "                total_loss += actual_loss\n",
        "                num_batches += 1\n",
        "\n",
        "                if num_batches % 5 == 0:\n",
        "                    print(f\"Batch {i}: Loss = {actual_loss:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch all exceptions during training, assuming memory/gradient issue\n",
        "            print(f\"FAILURE at Batch {i}: {e}. Retrying after aggressive cleanup...\")\n",
        "            torch.cuda.empty_cache()\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "    # End of Epoch Stats\n",
        "    avg_loss = total_loss / max(1, num_batches)\n",
        "    print(f\"--- Epoch {epoch + 1} Complete. Avg Loss: {avg_loss:.4f} ---\")\n",
        "\n",
        "    save_path = os.path.join(SAVE_DIR, f\"sc_adapter_integrated_epoch_{epoch+1}.pt\")\n",
        "    torch.save(adapter.state_dict(), save_path)\n",
        "    print(f\"Saved checkpoint: {save_path}\")\n",
        "\n",
        "print(\"Training Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5705569",
      "metadata": {
        "scrolled": true,
        "id": "a5705569"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "def generate_sc_phenotype_icl(wrapper, prot_id_int, gene_vec_np, data_args, device):\n",
        "    \"\"\"Generates phenotype candidates by injecting the Gene Embedding into the ProCyon ICL input structure.\"\"\"\n",
        "    wrapper.eval()\n",
        "\n",
        "    # --- A. Generate the working ICL Input Dictionary ---\n",
        "    input_simple = create_caption_input_simple(\n",
        "        input_aaseq_ids=[prot_id_int],\n",
        "        data_args=data_args,\n",
        "        instruction_source_dataset=\"uniprot\",\n",
        "        instruction_source_relation=\"all\",\n",
        "        aaseq_type=\"protein\",\n",
        "        task_type=\"caption\",\n",
        "        icl_example_number=1,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # --- B. Calculate Gene Embedding ---\n",
        "    main_device = wrapper.procyon.text_encoder.model.device\n",
        "    g_feat = torch.tensor(gene_vec_np, dtype=torch.float).unsqueeze(0).to(main_device).bfloat16()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        g_emb = wrapper.adapter(g_feat)\n",
        "\n",
        "    # --- C. INJECT THE GENE EMBEDDING ---\n",
        "    original_instruction = input_simple['instructions'][0]\n",
        "    new_instruction = \"Context: <|gene|>\\n\" + original_instruction.replace(\"Protein: <|protein|>\", \"Protein: <|protein|>\")\n",
        "    input_simple['instructions'][0] = new_instruction\n",
        "\n",
        "    gene_emb_numpy = g_emb.squeeze(0).cpu().float().numpy()\n",
        "\n",
        "    input_simple['data']['gene'] = [gene_emb_numpy]\n",
        "    input_simple['input']['gene'] = [[0]]\n",
        "\n",
        "    # --- D. Run Generation ---\n",
        "    text_gen_args = {\n",
        "        \"method\": \"beam\", \"max_len\": 200, \"beam_size\": 20,\n",
        "        \"beam_group_size\": 2, \"diversity_penalty\": 0.8,\n",
        "    }\n",
        "\n",
        "    out_tokens, log_probs, output_logits, out_text = wrapper.procyon.generate(\n",
        "        inputs=input_simple,\n",
        "        aaseq_type=\"protein\",\n",
        "        **text_gen_args\n",
        "    )\n",
        "\n",
        "    output_phenotypes = [\n",
        "        phen for i, phen in enumerate(out_text[0]) if i % text_gen_args[\"beam_group_size\"] == 0\n",
        "    ]\n",
        "    # Return the full tensor embedding for the QA step\n",
        "    return output_phenotypes, input_simple, g_emb\n",
        "\n",
        "def predict_phenotype_score_icl(wrapper, prot_id_int, g_emb_vector, data_args, candidate_text, device):\n",
        "    \"\"\"QA Step: Scores the candidate text using the integrated model with ICL.\"\"\"\n",
        "    wrapper.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use the ProCyon QA helper for scoring\n",
        "        input_qa_simple = create_qa_input_simple(\n",
        "            input_aaseq_ids=[prot_id_int],\n",
        "            data_args=data_args,\n",
        "            input_description=candidate_text,\n",
        "            instruction_source_dataset=\"uniprot\",\n",
        "            instruction_source_relation=\"all\",\n",
        "            aaseq_type=\"protein\",\n",
        "            icl_example_number=1,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        gene_emb_numpy = g_emb_vector.squeeze(0).cpu().float().numpy()\n",
        "\n",
        "        input_qa_simple['data']['gene'] = [gene_emb_numpy]\n",
        "        input_qa_simple['input']['gene'] = [[0]]\n",
        "\n",
        "        # Run QA model\n",
        "        qa_model = ProCyonQAInference(wrapper.procyon, device=device)\n",
        "        model_qa_out = qa_model(input_qa_simple)\n",
        "\n",
        "        # Extract \"Yes\" Probability\n",
        "        yes_prob = model_qa_out[\"pred\"][0, qa_model.yes_token].item()\n",
        "\n",
        "        return yes_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247b27c3",
      "metadata": {
        "id": "247b27c3"
      },
      "outputs": [],
      "source": [
        "test_index = 1\n",
        "sample_row = df_clean.iloc[test_index]\n",
        "\n",
        "gene_name = sample_row['gene_name']\n",
        "sample_gene_vec = sample_row['vec']\n",
        "sample_prot_id = int(sample_row['procyon_id'])\n",
        "true_summary = sample_row['summary']\n",
        "TARGET_UNIPROT_ID = sample_row['uniprot_id']\n",
        "\n",
        "# Define the label dynamically\n",
        "protein_name_label = f\"UniProt ID: {TARGET_UNIPROT_ID} (Gene: {gene_name})\"\n",
        "\n",
        "print(f\"\\n=== Gene-Augmented Phenotype Generation for {protein_name_label} ===\")\n",
        "print(f\"Gene Context Summary: {true_summary[:100]}...\\n\")\n",
        "\n",
        "print(\"1. Generating candidates with ICL Injection (Gene-Augmented)...\")\n",
        "\n",
        "# Use the established functions\n",
        "candidates, qa_input_template, g_emb_vector = generate_sc_phenotype_icl(\n",
        "    wrapper, sample_prot_id, sample_gene_vec, data_args, device\n",
        ")\n",
        "\n",
        "print(\"2. Scoring candidates (QA Check)...\")\n",
        "\n",
        "results = []\n",
        "for i, query_text in enumerate(candidates):\n",
        "    yes_prob = predict_phenotype_score_icl(\n",
        "        wrapper, sample_prot_id, g_emb_vector, data_args, query_text, device\n",
        "    )\n",
        "\n",
        "    results.append({ \"phenotype\": query_text, \"yes_prob\": yes_prob })\n",
        "\n",
        "# Display Results\n",
        "results.sort(key=lambda x: x['yes_prob'], reverse=True)\n",
        "\n",
        "for i, res in enumerate(results):\n",
        "    print(f\"\\nRANK {i+1} (Confidence: {res['yes_prob']:.4f}) {'-'*30}\")\n",
        "    print(res['phenotype'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mmc43gg-vHTI"
      },
      "id": "mmc43gg-vHTI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install bert-score\n",
        "!pip uninstall -y flash_attn"
      ],
      "metadata": {
        "id": "t7yaf6ztvuAv"
      },
      "id": "t7yaf6ztvuAv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "Pa2HVmPnqSV1"
      },
      "id": "Pa2HVmPnqSV1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_Q9UKD2 = [\"Component of the ribosome assembly machinery. Nuclear paralog of the ribosomal protein P0, it binds pre-60S subunits at an early stage of assembly in the nucleolus, and is replaced by P0 in cytoplasmic pre-60S subunits and mature 80S ribosomes.\"]\n",
        "phenotypes_with_genes_Q9UKD2 = [\"Required for pre-rRNA splicing as component of the spliceosome. Binds to the 5' splice site of pre-18S ribosomal RNA by RNA polymerase I, which is a component of the 5S rRNA.\"]\n",
        "phenotypes_only_protein_Q9UKD2 = [\"Involved in pre-mRNA splicing as component of the spliceosome. Binds to the 5S rRNA. Binds to the 5S rRNA. Binds to the 5S rRNA. Binds to the 5S rRNA.\"]\n",
        "#########################################\n",
        "ground_truth_Q96L58 = [\"Beta-1,3-galactosyltransferase that transfers galactose from UDP-galactose to substrates with a terminal beta-linked galactose residue. Has a preference for galactose-beta-1,4-xylose that is found in the linker region of glycosaminoglycans, such as heparan sulfate and chondroitin sulfate. Has no activity towards substrates with terminal glucosamine or galactosamine residues.\"]\n",
        "phenotypes_with_genes_Q96L58 = [\"Component of the V-ATPase V-ATPase family, which mediates the production of phosphatidylethanolamine (PE) to phosphatidic acid (PA) to produce lysophosphatidylserine (PS) to form adenylylsulfate (Glc) and glycerol (By similarity).\"]\n",
        "phenotypes_only_protein_Q96L58 = [\"Phosphatidylserine (PS) phosphatidylserine (PS) and phosphatidylethanolamine (PE), phosphatidylcholine (PC) and phosphatidylethanolamine (PE), phosphatidylserine and phosphatidylethanolamine (PE), phosphatidylserine and sphingosine 1-phosphate (PtdIns(4)P).\"]"
      ],
      "metadata": {
        "id": "cG8-v_hxI0r6"
      },
      "id": "cG8-v_hxI0r6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1423ea58",
      "metadata": {
        "id": "1423ea58"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "'''\n",
        "Get the BERT Score for a given protein\n",
        "'''\n",
        "def get_bertscore(g, p, gt):\n",
        "  \"\"\"\n",
        "  g: phenotype described using gene embeddings\n",
        "  p: phenotype described without gene embeddings\n",
        "  gt: ground truth val\n",
        "  \"\"\"\n",
        "  scorer = BERTScorer(lang=\"en\",\n",
        "                      model_type=\"allenai/scibert_scivocab_uncased\",\n",
        "                      num_layers=8,\n",
        "                      rescale_with_baseline=False)\n",
        "\n",
        "  gene_score = scorer.score(g, gt)\n",
        "  protein_score = scorer.score(p, gt)\n",
        "\n",
        "  return gene_score[2], protein_score[2]\n",
        "\n",
        "#Correct examples:\n",
        "g_score_Q9UKD2, p_score_Q9UKD2 = get_bertscore(phenotypes_with_genes_Q9UKD2, phenotypes_only_protein_Q9UKD2, ground_truth_Q9UKD2)\n",
        "g_score_Q96L58, p_score_Q96L58 = get_bertscore(phenotypes_with_genes_Q96L58, phenotypes_only_protein_Q96L58, ground_truth_Q96L58)\n",
        "\n",
        "print(f\"Bert Score for phenotypes of Q9UKD2 with gene embeddings: {g_score_Q9UKD2}\")\n",
        "print(f\"Bert Score for phenotyes of Q9UKD2 with ProCyon protein embeddings: {p_score_Q9UKD2}\")\n",
        "\n",
        "print(f\"Bert Score for phenotypes of Q96L58 with gene embeddings: {g_score_Q96L58}\")\n",
        "print(f\"Bert Score for phenotyes of Q96L58 with ProCyon protein embeddings: {p_score_Q96L58}\")\n",
        "\n",
        "#Procyon does a sample of 1000 for their bootstrap CI so we use that as our default\n",
        "# However, since we are operating on a smaller scale we only calculate ci on a few samples\n",
        "'''\n",
        "Calculates the bootstrap ci for given bertscores\n",
        "'''\n",
        "def compute_bootstrap_ci(scores, samples=1000):\n",
        "  \"\"\"\n",
        "    scores: Berstscores\n",
        "    samples: number of samples to bootstrap\n",
        "  \"\"\"\n",
        "  means = []\n",
        "  for _ in range(samples):\n",
        "      sample = np.random.choice(scores, size=len(scores), replace=True)\n",
        "      means.append(sample.mean())\n",
        "\n",
        "  lower = np.percentile(means, 2.5)\n",
        "  upper = np.percentile(means, 97.5)\n",
        "\n",
        "  return scores.mean(), lower, upper\n",
        "\n",
        "g_scores = np.array([g_score_Q9UKD2.item(), g_score_Q96L58.item()])\n",
        "print(f\"g_scores: {g_scores}\")\n",
        "\n",
        "p_scores = np.array([p_score_Q9UKD2.item(), p_score_Q96L58.item()])\n",
        "print(f\"p_scores: {p_scores}\")\n",
        "\n",
        "#These should just be the same as the initial BERTScore since we only have 2 proteins to sample from\n",
        "mean_g, low_g, high_g = compute_bootstrap_ci(g_scores, 1)\n",
        "print(f\"Mean for Gene Model: {mean_g:.6f} (95% CI: {low_g:.6f} - {high_g:.6f})\")\n",
        "\n",
        "mean_p, low_p, high_p = compute_bootstrap_ci(p_scores, 1)\n",
        "print(f\"Mean Protein Model: {mean_p:.6f} (95% CI: {low_p:.6f} - {high_p:.6f})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "import gc\n",
        "import random\n",
        "\n",
        "model = None\n",
        "\"\"\"\n",
        "Ensure LLM is only instantiated once per session, not once per judge_phenotype call\n",
        "\"\"\"\n",
        "def get_pipeline():\n",
        "    global model\n",
        "    if model is None:\n",
        "        model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "        model = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            dtype=\"auto\",\n",
        "            return_full_text=False,\n",
        "            temperature=0.01,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "    return model\n",
        "\"\"\"\n",
        "Prevents memory from filling up\n",
        "\"\"\"\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\"\"\"\n",
        "LLM as judge approach for rating the biological accuracy/quality of the\n",
        "\"\"\"\n",
        "def judge_phenotypes(ground_truth, a, b):\n",
        "    '''\n",
        "    ground_truth: GT phenotype description\n",
        "    a: generated phenotype description (ProCyon or Procupine)\n",
        "    b: generated phenotype description (ProCyon or Procupine)\n",
        "    '''\n",
        "    clear_gpu_memory()\n",
        "    pipe = get_pipeline()\n",
        "\n",
        "    options = {\"a\": a, \"b\": b}\n",
        "    #Shuffle the options to avoid bias\n",
        "    keys = list(options.keys())\n",
        "    random.shuffle(keys)\n",
        "    shuffled_a = options[keys[0]]\n",
        "    shuffled_b = options[keys[1]]\n",
        "\n",
        "    message = f\"You are an expert biologist. Compare these two phenotype descriptions against the Ground Truth. Ground Truth: {ground_truth} Option A: {shuffled_a} Option B: {shuffled_b} Which option better captures the specific biological context (tissue, regulation, pathway)? Provide a single word response: ['Option A', 'Option B', 'Tie'] and no additional explination.\"\n",
        "    #Limit the number of tokens to prevent the LLM from providing a run on explination.\n",
        "    outputs = pipe(\n",
        "        message,\n",
        "        max_new_tokens=10,\n",
        "    )\n",
        "\n",
        "    return outputs[0]['generated_text'].split('\\n')[0].strip(' ')\n",
        "\n",
        "\n",
        "#Test on two proteins\n",
        "Q9UKD2=judge_phenotypes(ground_truth_Q9UKD2 ,phenotypes_with_genes_Q9UKD2, phenotypes_only_protein_Q9UKD2)\n",
        "Q96L58=judge_phenotypes(ground_truth_Q96L58 ,phenotypes_with_genes_Q96L58, phenotypes_only_protein_Q96L58)\n",
        "\n",
        "print(\"Results for Q9UKD2: \" + Q9UKD2)\n",
        "print(\"Results for Q96L58: \" + Q96L58)"
      ],
      "metadata": {
        "id": "AOuK9GXsqUlV"
      },
      "id": "AOuK9GXsqUlV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ProCyon",
      "language": "python",
      "name": "procyon"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}